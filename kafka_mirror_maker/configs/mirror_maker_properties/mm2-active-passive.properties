# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# See org.apache.kafka.clients.consumer.ConsumerConfig for more details

# -----------------------------------------------------------------------------
# MirrorMaker 2.0 configuration file
# Run with ./bin/connect-mirror-maker.sh connect-mirror-maker.properties
# -----------------------------------------------------------------------------

# Define the clusters to be used
clusters = A, B

# Connection info for each cluster
A.bootstrap.servers = kafka1-a:9092,kafka2-a:9092,kafka3-a:9092
B.bootstrap.servers = kafka1-b:9092,kafka2-b:9092,kafka3-b:9092

# Enable/disable replication flows
A->B.enabled = true
B->A.enabled = false

# Replication topic filter: use regex to define what gets mirrored
A->B.topics = .*

# Offset and heartbeat management
A->B.exclude.internal.topics = true
A->B.sync.group.offsets.enabled = true
A->B.emit.heartbeats.enabled = true
A->B.emit.checkpoints.enabled = true

# Replication policy - keep topic names unchanged
replication.policy.class = org.apache.kafka.connect.mirror.IdentityReplicationPolicy

# Converters (raw bytes)
key.converter = org.apache.kafka.connect.converters.ByteArrayConverter
value.converter = org.apache.kafka.connect.converters.ByteArrayConverter
header.converter = org.apache.kafka.connect.converters.ByteArrayConverter

# Interval tuning (in seconds)
emit.heartbeats.interval.seconds = 1
emit.checkpoints.interval.seconds = 1
refresh.topics.interval.seconds = 5
refresh.groups.interval.seconds = 5
sync.topic.configs.interval.seconds = 5
sync.group.offsets.interval.seconds = 5

# Internal task tuning
tasks.max = 12

# Replication factor for new and internal topics
# can use -1 for auto-detection based on cluster size
replication.factor = 3
checkpoints.topic.replication.factor = 3
heartbeats.topic.replication.factor = 3
offset-syncs.topic.replication.factor = 3

# Replication factor for Kafka Connect internal topics
config.storage.replication.factor = 3
offset.storage.replication.factor = 3
status.storage.replication.factor = 3

# Flush timeout for offset commits
offset.flush.timeout.ms = 10800000

# -----------------------------------------------------------------------------
# Optional: Enable producer idempotence
producer.override.enable.idempotence = true
producer.override.acks = all
producer.override.retries = 2147483647
producer.override.max.in.flight.requests.per.connection = 5
producer.override.retry.backoff.ms = 1000
producer.override.request.timeout.ms = 5000
producer.override.delivery.timeout.ms=120000
# -----------------------------------------------------------------------------
# SASL / SCRAM example configuration
# B.security.protocol = SASL_PLAINTEXT
# B.sasl.mechanism = SCRAM-SHA-256
# B.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="xxxxx";

# A.security.protocol = SASL_PLAINTEXT
# A.sasl.mechanism = SCRAM-SHA-256
# A.sasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="yyyyy";
